{
  "hash": "500705578e9fa65b7ff18bbdea66faca",
  "result": {
    "markdown": "---\ntitle: Clustering in Data Analytics\nauthor: Sahil Sharma\ndate: '2023-11-07'\ncategories:\n  - Analysis\n  - Visualization\n  - Clustering\nimage: clustering.png\n---\n\n# Clustering Iris Data with K-Means Algorithm\n\n\n### Introduction\n\nClustering is an unsupervised learning technique that groups data points together based on their similarities. It is a useful tool for exploring data and identifying patterns. The k-means algorithm is a popular clustering algorithm that works by assigning data points to a predefined number of clusters (k).\n\n### Why Clustering Matters ?\n \nClustering is a versatile tool used for a wide range of applications, including:\n\nCustomer segmentation: Clustering can be used to identify groups of customers with similar characteristics, enabling targeted marketing campaigns and personalized product recommendations.\n\nImage segmentation: Clustering can be used to segment images into different regions, such as identifying objects or segmenting tissues in medical imaging.\n\nAnomaly detection: Clustering can be used to identify outliers or anomalous data points that deviate from the typical patterns in the data.\n\nExploratory data analysis: Clustering can be used to explore and visualize the underlying structure and relationships within a dataset.\n\n### Common Clustering Algorithms\n\nNumerous clustering algorithms exist, each with its own strengths and limitations. Some popular clustering algorithms include:\n\nK-means clustering: This algorithm partitions the data into a predefined number of clusters (k) by iteratively assigning data points to the nearest cluster centroid.\n\nHierarchical clustering: This algorithm builds a hierarchy of clusters by recursively merging or splitting data points based on their similarity.\n\nDensity-based spatial clustering of applications with noise (DBSCAN): This algorithm identifies clusters based on the density of data points, marking outliers as points that lie in low-density regions.\n\nIn this blog post, we will use the k-means algorithm to cluster the Iris dataset. The Iris dataset is a popular benchmark dataset that consists of 150 samples of iris flowers, each belonging to one of three species: Iris setosa, Iris versicolor, and Iris virginica. Each sample is characterized by four features: sepal length, sepal width, petal length, and petal width.\n\n### Importing Libraries\n\nWe will need to import the following libraries into our Python script:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot as plt\n```\n:::\n\n\n### Loading and Preprocessing Data\n\nWe will load the Iris dataset using pandas and preprocess the data by standardizing the features.\n\n### Load the Iris dataset\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv('iris.csv')\n```\n:::\n\n\n### Preprocess the data\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n```\n:::\n\n\n### Clustering with K-Means\n\nWe will now use the k-means algorithm to cluster the data. We will set the number of clusters to three, as there are three species of iris flowers in the dataset.\n\n### Create a KMeans object with k=3 clusters\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=3)\n```\n:::\n\n\n### Fit the KMeans model to the data\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nkmeans.fit(data_scaled)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/sahilsharma/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n### Evaluating Clustering Results\n\nWe can evaluate the clustering results by calculating the silhouette score. The silhouette score is a measure of how well each data point is clustered.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\n```\n:::\n\n\n### Calculate the silhouette score\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsilhouette_avg = silhouette_score(data_scaled, kmeans.labels_)\nprint('Silhouette score:', silhouette_avg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette score: 0.45897178670187166\n```\n:::\n:::\n\n\n### Vizualising CLusters\n\nWe can visualize the clusters by plotting the data points in two dimensions.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Extract the first two principal components\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_data = pca.fit_transform(data_scaled)\n\n# Plot the data points in two dimensions\nplt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans.labels_)\nplt.title('Clustered Iris Data')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=569 height=431}\n:::\n:::\n\n\n### Conclusion\n\nIn this blog post, we have used the k-means algorithm to cluster the Iris dataset. We have also evaluated the clustering results and visualized the clusters. Clustering is a powerful tool for data exploration, pattern recognition, and group identification. By mastering clustering techniques, we can uncover hidden structures and relationships within data, gain valuable insights, and make informed decisions in various domains.\n\n**References**\n\n* [scikit-learn documentation](https://scikit-learn.org/stable/index.html)\n* [Iris dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}